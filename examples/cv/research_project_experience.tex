%-------------------------------------------------------------------------------
%	SECTION TITLE
%-------------------------------------------------------------------------------
\cvsection{Research Project Experience}


%-------------------------------------------------------------------------------
%	CONTENT
%-------------------------------------------------------------------------------
\newcommand{\urban}{\href{https://urobot.kaist.ac.kr/}{Urban Robotics Lab.}}
\begin{cventries}
%---------------------------------------------------------

  % 라스트마일
  \cventry
    {Supported by the Ministry of Trade, Industry \& Energy (MOTIE), Republic of Korea} % Job title
    {Development of Artificial Intelligence Robot Autonomous Navigation Technology for Agile Movement in Crowded Space} % Organization
    {\urban} % Location
    {Mar. 2020 - June 2023} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        \item Was in charge of dynamic object detection in order to generate static maps and perform moving-object-robust SLAM in dynamic environments
        \item Keywords: SLAM, 3D LiDAR, Static Map Building, OpenCV, C++, ROS
      \end{cvitems}
    }

%---------------------------------------------------------
  % LG Semantic SLAM
  \cventry
    {Supported by LG Electronics} % Job title
    {Study of Semantic SLAM Towards Spatial AI Technologies} % Organization
    {\urban} % Location
    {Mar. 2022 - Dec. 2022} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        \item Studied various deep learning-aided SLAM methods, such as Kimera and Hydra
        \item Run semantic SLAM in our real-world data to check the feasibility
        \item Keywords: 3D semantic mapping, Semantic SLAM
      \end{cvitems}
    }

  % 현대 케피코
  \cventry
    {Supported by Hyundai Kefico} % Job title
    {Deep Learning-Based Depth Prediction Using a Mono Camera and 2D LiDAR Sensor} % Organization
    {\urban} % Location
    {Mar. 2022 - Nov. 2022} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        \item Was in charge of development of deep learning-based depth prediction
        \item Studied various calibration methods (2D LiDAR-to-camera, 3D LiDAR-to-IMU, camera-to-IMU)
        \item Run deep learning models on NVIDIA Xavier
        \item Keywords: Real-time deep learning, Depth prediction, Calibration, PyTorch, OpenCV, C++, ROS
      \end{cvitems}
    }

  % VPR
  \cventry
    {Supported by KAIST Institute for Security Convergence Research} % Job title
    {A Study on the Visual Place Recognition in Multiple Photos} % Organization
    {\urban} % Location
    {Mar. 2020 - Dec. 2020} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        \item Studied visual place recognition application. Struggled to improve performance of the SOTA deep learning-based VPR approaches
        \item Keywords: Visual place recognition, Deep learning, Semantic segmentation, Inpainting, Python, PyTorch
      \end{cvitems}
    }

  % 이동지능
  \cventry
    {Supported by the Ministry of Trade, Industry \& Energy (MOTIE)} % Job title
    {Development of Robot Intelligence Technology for Mobility with Learning Capability Toward Robust and Seamless Indoor and Outdoor Autonomous Navigation} % Organization
    {\urban} % Location
    {Mar. 2020 - Sep. 2020} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        \item Developed SLAM algorithm applied to mobile robots
        \item Was in charge of SLAM, static map building in low-dynamic environments, and movable area prediction
        \item Keywords: SLAM, 3D LiDAR, Registration, Static map building, Mobile robots, C++, ROS
      \end{cvitems}
    }

  \cventry
    {Supported by Samsung Electronics Co., Ltd.} % Job title
    {Indoor Navigation of Mobile Robots using Deep Learning-based Object Recognition} % Organization
    {\urban} % Location
    {Jan. 2019 - Sep. 2019} % Date(s)
    {
      \begin{cvitems} % Description(s) of tasks/responsibilities
        \item Developed SLAM and perception algorithms applied to robot cleaners for achieving robust navigation in indoor environments
        \item Was in charge of depth prediction using a 2D LiDAR sensor and a monocular camera for collision avoidance of mobile robots via deep learning
        \item Keywords: 2D LiDAR, Sensor Fusion, Deep Learning, Mobile Robots, PyTorch, ROS
      \end{cvitems}
    }

    \cventry
      {Supported by IITP, which is a government-affiliated organization} % Job title
      {IITP Artificial Intelligence R\&D Grand Challenge: Track 4, Intelligent Control} % Organization
      {\urban} % Location
      {Jan. 2019 - Jun. 2019} % Date(s)
      {
        \begin{cvitems} % Description(s) of tasks/responsibilities
          \item Was in charge of the task of a drone passing through windows
          \item Implemented RGB-D camera-based path planning\&following. Participated in applying VIO to estimate odometry of UAV
          \item Keywords: VIO, Path planning and following, Projective geometry, OpenCV, ROS
        \end{cvitems}
      }

    \cventry
      {Outsourced by Pixoneer Geomatics and Agency for Defense Development} % Job title
      {Machine Learning-Based Classification of Small Object captured by Unmanned Aerial Vehicle} % Organization
      {\urban} % Location
      {Jan. 2018 - Dec. 2019} % Date(s)
      {
        \begin{cvitems} % Description(s) of tasks/responsibilities
          \item Developed both SVM-based and Deep Learning-based classification algorithms
          \item Implemented HOG-LBP for input to SVM and engaged in designing novel Deep Learning architecture.
          \item Keywords: Deep Learning, SVM, HOG-LBP, Classification, Python, PyTorch
        \end{cvitems}
      }

    \cventry
      {Supported by Ministry of Trade, Industry, and Energy} % Job title
      {Range-Only SLAM in Complex Disaster Situation} % Organization
      {\urban} % Location
      {Jan. 2018 - Dec. 2018} % Date(s)
      {
        \begin{cvitems} % Description(s) of tasks/responsibilities
          \item Implemented monte carlo localization (MCL) using range measurements by ultra-wideband (UWB) sensors for UAV from scratch single-handed
          \item Struggled to cover None-line-of-sight (NLOS) issues.
          \item Keywords: MCL, Beacon-based localization, UWB sensors, NLOS, ROS
        \end{cvitems}
      }

\end{cventries}
